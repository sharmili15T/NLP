{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34970df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955abca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7cbc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\tsharmili\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d70d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Natural Language processing(NLP), describes the interaction between human language and computers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bcaf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing(nlp), describes the interaction between human language and computers'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4125ebbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NATURAL LANGUAGE PROCESSING(NLP), DESCRIBES THE INTERACTION BETWEEN HUMAN LANGUAGE AND COMPUTERS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196c6713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "imdb = pd.read_csv('imdb_sentiment.csv')\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5933801",
   "metadata": {},
   "source": [
    "\n",
    "Converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd56400c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  a very, very, very slow-moving, aimless movie ...          0\n",
       "1  not sure who was more lost - the flat characte...          0\n",
       "2  attempting artiness with black & white and cle...          0\n",
       "3       very little music or anything to speak of.            0\n",
       "4  the best scene in the movie was when gerardo i...          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['review']=imdb['review'].apply(lambda x:x.lower())\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc551e",
   "metadata": {},
   "source": [
    "Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6fee90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c621efda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language processingNLP describes the interaction between human language and computers'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'[^\\w\\s]','',text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64939327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>a very very very slowmoving aimless movie abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "      <td>not sure who was more lost  the flat character...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>attempting artiness with black  white and clev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "      <td>very little music or anything to speak of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "      <td>1</td>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  a very, very, very slow-moving, aimless movie ...          0   \n",
       "1  not sure who was more lost - the flat characte...          0   \n",
       "2  attempting artiness with black & white and cle...          0   \n",
       "3       very little music or anything to speak of.            0   \n",
       "4  the best scene in the movie was when gerardo i...          1   \n",
       "\n",
       "                                               clean  \n",
       "0  a very very very slowmoving aimless movie abou...  \n",
       "1  not sure who was more lost  the flat character...  \n",
       "2  attempting artiness with black  white and clev...  \n",
       "3        very little music or anything to speak of    \n",
       "4  the best scene in the movie was when gerardo i...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['clean'] = imdb['review'].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a552fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44329a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'processingNLP',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'between',\n",
       " 'human',\n",
       " 'language',\n",
       " 'and',\n",
       " 'computers']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97d38dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Natural Language processing(NLP), describes the interaction between human language and computers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea3d405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing(nlp), describes the interaction between human language and computers'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3024033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing(nlp), describes interaction human language computers'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join([x for x in text.split() if x not in stop])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b32dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb['clean']= imdb['clean'].apply(lambda x:' '.join([x for x in x.split() if x not in stop]))\n",
    "                                   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29cba784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slowmoving aimless movie distressed drifting y...\n",
       "1    sure lost flat characters audience nearly half...\n",
       "2    attempting artiness black white clever camera ...\n",
       "3                          little music anything speak\n",
       "4    best scene movie gerardo trying find song keep...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49a7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e43b249b",
   "metadata": {},
   "source": [
    "### Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31e0675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad1719ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad229ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hospial is far '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'hospial is far '\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daf09ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = TextBlob(text2).correct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeaa4fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"hospital is far \")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f9bee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb['clean'] = imdb['clean'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b740fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slowmoving aimless movie distressed drifting y...\n",
       "1    sure lost flat characters audience nearly half...\n",
       "2    attempting artless black white clever camera a...\n",
       "3                          little music anything speak\n",
       "4    best scene movie gerard trying find song keeps...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c483c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b29f86a",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6b3bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text =\"\"\"Because of problems with her eyesight, rey the African penguin had issues with swimming. That’s unusual for a penguin, and presented a big challenge for our aviculture team to help Rey overcome her hesitancy. Slowly and steadily, we trained her to be comfortable feeding in the water like the rest of the penguin colony. The aviculturists also trained Rey to accept daily eye drops from them as part of her special health care. Rey already had good relationships with some staff, and was comfortable with them handling her. Senior Aviculturist Kim Fukuda says the team built on those bonds to get Rey used to receiving the eye drops. \"She knows the routine,\" Kim says. \"I usually give her the eye drops in one area of the exhibit after all the penguins get their vitamins. When that happens, she runs over there and waits for me.\" Rosa, our oldest sea otter, has very limited eyesight, among other health issues. The sea otter team had already trained Rosa so they could examine her eyes, and built on that trust to include administering the eye drops she needs.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19a56834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Because',\n",
       " 'of',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'her',\n",
       " 'eyesight,',\n",
       " 'rey',\n",
       " 'the',\n",
       " 'African',\n",
       " 'penguin',\n",
       " 'had',\n",
       " 'issues',\n",
       " 'with',\n",
       " 'swimming.',\n",
       " 'That’s',\n",
       " 'unusual',\n",
       " 'for',\n",
       " 'a',\n",
       " 'penguin,',\n",
       " 'and',\n",
       " 'presented',\n",
       " 'a',\n",
       " 'big',\n",
       " 'challenge',\n",
       " 'for',\n",
       " 'our',\n",
       " 'aviculture',\n",
       " 'team',\n",
       " 'to',\n",
       " 'help',\n",
       " 'Rey',\n",
       " 'overcome',\n",
       " 'her',\n",
       " 'hesitancy.',\n",
       " 'Slowly',\n",
       " 'and',\n",
       " 'steadily,',\n",
       " 'we',\n",
       " 'trained',\n",
       " 'her',\n",
       " 'to',\n",
       " 'be',\n",
       " 'comfortable',\n",
       " 'feeding',\n",
       " 'in',\n",
       " 'the',\n",
       " 'water',\n",
       " 'like',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'penguin',\n",
       " 'colony.',\n",
       " 'The',\n",
       " 'aviculturists',\n",
       " 'also',\n",
       " 'trained',\n",
       " 'Rey',\n",
       " 'to',\n",
       " 'accept',\n",
       " 'daily',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'from',\n",
       " 'them',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'her',\n",
       " 'special',\n",
       " 'health',\n",
       " 'care.',\n",
       " 'Rey',\n",
       " 'already',\n",
       " 'had',\n",
       " 'good',\n",
       " 'relationships',\n",
       " 'with',\n",
       " 'some',\n",
       " 'staff,',\n",
       " 'and',\n",
       " 'was',\n",
       " 'comfortable',\n",
       " 'with',\n",
       " 'them',\n",
       " 'handling',\n",
       " 'her.',\n",
       " 'Senior',\n",
       " 'Aviculturist',\n",
       " 'Kim',\n",
       " 'Fukuda',\n",
       " 'says',\n",
       " 'the',\n",
       " 'team',\n",
       " 'built',\n",
       " 'on',\n",
       " 'those',\n",
       " 'bonds',\n",
       " 'to',\n",
       " 'get',\n",
       " 'Rey',\n",
       " 'used',\n",
       " 'to',\n",
       " 'receiving',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops.',\n",
       " '\"She',\n",
       " 'knows',\n",
       " 'the',\n",
       " 'routine,\"',\n",
       " 'Kim',\n",
       " 'says.',\n",
       " '\"I',\n",
       " 'usually',\n",
       " 'give',\n",
       " 'her',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'in',\n",
       " 'one',\n",
       " 'area',\n",
       " 'of',\n",
       " 'the',\n",
       " 'exhibit',\n",
       " 'after',\n",
       " 'all',\n",
       " 'the',\n",
       " 'penguins',\n",
       " 'get',\n",
       " 'their',\n",
       " 'vitamins.',\n",
       " 'When',\n",
       " 'that',\n",
       " 'happens,',\n",
       " 'she',\n",
       " 'runs',\n",
       " 'over',\n",
       " 'there',\n",
       " 'and',\n",
       " 'waits',\n",
       " 'for',\n",
       " 'me.\"',\n",
       " 'Rosa,',\n",
       " 'our',\n",
       " 'oldest',\n",
       " 'sea',\n",
       " 'otter,',\n",
       " 'has',\n",
       " 'very',\n",
       " 'limited',\n",
       " 'eyesight,',\n",
       " 'among',\n",
       " 'other',\n",
       " 'health',\n",
       " 'issues.',\n",
       " 'The',\n",
       " 'sea',\n",
       " 'otter',\n",
       " 'team',\n",
       " 'had',\n",
       " 'already',\n",
       " 'trained',\n",
       " 'Rosa',\n",
       " 'so',\n",
       " 'they',\n",
       " 'could',\n",
       " 'examine',\n",
       " 'her',\n",
       " 'eyes,',\n",
       " 'and',\n",
       " 'built',\n",
       " 'on',\n",
       " 'that',\n",
       " 'trust',\n",
       " 'to',\n",
       " 'include',\n",
       " 'administering',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'she',\n",
       " 'needs.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a4b7101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Because',\n",
       " 'of',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'her',\n",
       " 'eyesight',\n",
       " 'rey',\n",
       " 'the',\n",
       " 'African',\n",
       " 'penguin',\n",
       " 'had',\n",
       " 'issues',\n",
       " 'with',\n",
       " 'swimming',\n",
       " 'That',\n",
       " 's',\n",
       " 'unusual',\n",
       " 'for',\n",
       " 'a',\n",
       " 'penguin',\n",
       " 'and',\n",
       " 'presented',\n",
       " 'a',\n",
       " 'big',\n",
       " 'challenge',\n",
       " 'for',\n",
       " 'our',\n",
       " 'aviculture',\n",
       " 'team',\n",
       " 'to',\n",
       " 'help',\n",
       " 'Rey',\n",
       " 'overcome',\n",
       " 'her',\n",
       " 'hesitancy',\n",
       " 'Slowly',\n",
       " 'and',\n",
       " 'steadily',\n",
       " 'we',\n",
       " 'trained',\n",
       " 'her',\n",
       " 'to',\n",
       " 'be',\n",
       " 'comfortable',\n",
       " 'feeding',\n",
       " 'in',\n",
       " 'the',\n",
       " 'water',\n",
       " 'like',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'penguin',\n",
       " 'colony',\n",
       " 'The',\n",
       " 'aviculturists',\n",
       " 'also',\n",
       " 'trained',\n",
       " 'Rey',\n",
       " 'to',\n",
       " 'accept',\n",
       " 'daily',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'from',\n",
       " 'them',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'her',\n",
       " 'special',\n",
       " 'health',\n",
       " 'care',\n",
       " 'Rey',\n",
       " 'already',\n",
       " 'had',\n",
       " 'good',\n",
       " 'relationships',\n",
       " 'with',\n",
       " 'some',\n",
       " 'staff',\n",
       " 'and',\n",
       " 'was',\n",
       " 'comfortable',\n",
       " 'with',\n",
       " 'them',\n",
       " 'handling',\n",
       " 'her',\n",
       " 'Senior',\n",
       " 'Aviculturist',\n",
       " 'Kim',\n",
       " 'Fukuda',\n",
       " 'says',\n",
       " 'the',\n",
       " 'team',\n",
       " 'built',\n",
       " 'on',\n",
       " 'those',\n",
       " 'bonds',\n",
       " 'to',\n",
       " 'get',\n",
       " 'Rey',\n",
       " 'used',\n",
       " 'to',\n",
       " 'receiving',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'She',\n",
       " 'knows',\n",
       " 'the',\n",
       " 'routine',\n",
       " 'Kim',\n",
       " 'says',\n",
       " 'I',\n",
       " 'usually',\n",
       " 'give',\n",
       " 'her',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'in',\n",
       " 'one',\n",
       " 'area',\n",
       " 'of',\n",
       " 'the',\n",
       " 'exhibit',\n",
       " 'after',\n",
       " 'all',\n",
       " 'the',\n",
       " 'penguins',\n",
       " 'get',\n",
       " 'their',\n",
       " 'vitamins',\n",
       " 'When',\n",
       " 'that',\n",
       " 'happens',\n",
       " 'she',\n",
       " 'runs',\n",
       " 'over',\n",
       " 'there',\n",
       " 'and',\n",
       " 'waits',\n",
       " 'for',\n",
       " 'me',\n",
       " 'Rosa',\n",
       " 'our',\n",
       " 'oldest',\n",
       " 'sea',\n",
       " 'otter',\n",
       " 'has',\n",
       " 'very',\n",
       " 'limited',\n",
       " 'eyesight',\n",
       " 'among',\n",
       " 'other',\n",
       " 'health',\n",
       " 'issues',\n",
       " 'The',\n",
       " 'sea',\n",
       " 'otter',\n",
       " 'team',\n",
       " 'had',\n",
       " 'already',\n",
       " 'trained',\n",
       " 'Rosa',\n",
       " 'so',\n",
       " 'they',\n",
       " 'could',\n",
       " 'examine',\n",
       " 'her',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'built',\n",
       " 'on',\n",
       " 'that',\n",
       " 'trust',\n",
       " 'to',\n",
       " 'include',\n",
       " 'administering',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'she',\n",
       " 'needs']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = re.findall(\"[\\w']+\",Text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14556856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Because',\n",
       " 'of',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'her',\n",
       " 'eyesight',\n",
       " ',',\n",
       " 'rey',\n",
       " 'the',\n",
       " 'African',\n",
       " 'penguin',\n",
       " 'had',\n",
       " 'issues',\n",
       " 'with',\n",
       " 'swimming',\n",
       " '.',\n",
       " 'That',\n",
       " '’',\n",
       " 's',\n",
       " 'unusual',\n",
       " 'for',\n",
       " 'a',\n",
       " 'penguin',\n",
       " ',',\n",
       " 'and',\n",
       " 'presented',\n",
       " 'a',\n",
       " 'big',\n",
       " 'challenge',\n",
       " 'for',\n",
       " 'our',\n",
       " 'aviculture',\n",
       " 'team',\n",
       " 'to',\n",
       " 'help',\n",
       " 'Rey',\n",
       " 'overcome',\n",
       " 'her',\n",
       " 'hesitancy',\n",
       " '.',\n",
       " 'Slowly',\n",
       " 'and',\n",
       " 'steadily',\n",
       " ',',\n",
       " 'we',\n",
       " 'trained',\n",
       " 'her',\n",
       " 'to',\n",
       " 'be',\n",
       " 'comfortable',\n",
       " 'feeding',\n",
       " 'in',\n",
       " 'the',\n",
       " 'water',\n",
       " 'like',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'penguin',\n",
       " 'colony',\n",
       " '.',\n",
       " 'The',\n",
       " 'aviculturists',\n",
       " 'also',\n",
       " 'trained',\n",
       " 'Rey',\n",
       " 'to',\n",
       " 'accept',\n",
       " 'daily',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'from',\n",
       " 'them',\n",
       " 'as',\n",
       " 'part',\n",
       " 'of',\n",
       " 'her',\n",
       " 'special',\n",
       " 'health',\n",
       " 'care',\n",
       " '.',\n",
       " 'Rey',\n",
       " 'already',\n",
       " 'had',\n",
       " 'good',\n",
       " 'relationships',\n",
       " 'with',\n",
       " 'some',\n",
       " 'staff',\n",
       " ',',\n",
       " 'and',\n",
       " 'was',\n",
       " 'comfortable',\n",
       " 'with',\n",
       " 'them',\n",
       " 'handling',\n",
       " 'her',\n",
       " '.',\n",
       " 'Senior',\n",
       " 'Aviculturist',\n",
       " 'Kim',\n",
       " 'Fukuda',\n",
       " 'says',\n",
       " 'the',\n",
       " 'team',\n",
       " 'built',\n",
       " 'on',\n",
       " 'those',\n",
       " 'bonds',\n",
       " 'to',\n",
       " 'get',\n",
       " 'Rey',\n",
       " 'used',\n",
       " 'to',\n",
       " 'receiving',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " '.',\n",
       " '``',\n",
       " 'She',\n",
       " 'knows',\n",
       " 'the',\n",
       " 'routine',\n",
       " ',',\n",
       " \"''\",\n",
       " 'Kim',\n",
       " 'says',\n",
       " '.',\n",
       " '``',\n",
       " 'I',\n",
       " 'usually',\n",
       " 'give',\n",
       " 'her',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'in',\n",
       " 'one',\n",
       " 'area',\n",
       " 'of',\n",
       " 'the',\n",
       " 'exhibit',\n",
       " 'after',\n",
       " 'all',\n",
       " 'the',\n",
       " 'penguins',\n",
       " 'get',\n",
       " 'their',\n",
       " 'vitamins',\n",
       " '.',\n",
       " 'When',\n",
       " 'that',\n",
       " 'happens',\n",
       " ',',\n",
       " 'she',\n",
       " 'runs',\n",
       " 'over',\n",
       " 'there',\n",
       " 'and',\n",
       " 'waits',\n",
       " 'for',\n",
       " 'me',\n",
       " '.',\n",
       " \"''\",\n",
       " 'Rosa',\n",
       " ',',\n",
       " 'our',\n",
       " 'oldest',\n",
       " 'sea',\n",
       " 'otter',\n",
       " ',',\n",
       " 'has',\n",
       " 'very',\n",
       " 'limited',\n",
       " 'eyesight',\n",
       " ',',\n",
       " 'among',\n",
       " 'other',\n",
       " 'health',\n",
       " 'issues',\n",
       " '.',\n",
       " 'The',\n",
       " 'sea',\n",
       " 'otter',\n",
       " 'team',\n",
       " 'had',\n",
       " 'already',\n",
       " 'trained',\n",
       " 'Rosa',\n",
       " 'so',\n",
       " 'they',\n",
       " 'could',\n",
       " 'examine',\n",
       " 'her',\n",
       " 'eyes',\n",
       " ',',\n",
       " 'and',\n",
       " 'built',\n",
       " 'on',\n",
       " 'that',\n",
       " 'trust',\n",
       " 'to',\n",
       " 'include',\n",
       " 'administering',\n",
       " 'the',\n",
       " 'eye',\n",
       " 'drops',\n",
       " 'she',\n",
       " 'needs',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token = word_tokenize(Text)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f9a2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50e219e8",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cc52511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e36cd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'becaus of problem with her eyesight, rey the african penguin had issu with swimming. that’ unusu for a penguin, and present a big challeng for our avicultur team to help rey overcom her hesitancy. slowli and steadily, we train her to be comfort feed in the water like the rest of the penguin colony. the aviculturist also train rey to accept daili eye drop from them as part of her special health care. rey alreadi had good relationship with some staff, and wa comfort with them handl her. senior aviculturist kim fukuda say the team built on those bond to get rey use to receiv the eye drops. \"she know the routine,\" kim says. \"i usual give her the eye drop in one area of the exhibit after all the penguin get their vitamins. when that happens, she run over there and wait for me.\" rosa, our oldest sea otter, ha veri limit eyesight, among other health issues. the sea otter team had alreadi train rosa so they could examin her eyes, and built on that trust to includ administ the eye drop she needs.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port = PorterStemmer()\n",
    "\n",
    "words =[]\n",
    "for word in Text.split():\n",
    "    words.append(port.stem(word))\n",
    "    \n",
    "Text2 = ' '.join(words)\n",
    "Text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13f682ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'becaus of problem with her eyesight, rey the african penguin had issu with swimming. that unusu for a penguin, and present a big challeng for our avicultur team to help rey overcom her hesitancy. slowli and steadily, we train her to be comfort feed in the water like the rest of the penguin colony. the aviculturist also train rey to accept daili eye drop from them as part of her special health care. rey alreadi had good relationship with some staff, and was comfort with them handl her. senior aviculturist kim fukuda say the team built on those bond to get rey use to receiv the eye drops. \"she know the routine,\" kim says. \"i usual give her the eye drop in one area of the exhibit after all the penguin get their vitamins. when that happens, she run over there and wait for me.\" rosa, our oldest sea otter, has veri limit eyesight, among other health issues. the sea otter team had alreadi train rosa so they could examin her eyes, and built on that trust to includ administ the eye drop she needs.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow = SnowballStemmer('english')\n",
    "\n",
    "w=[]\n",
    "\n",
    "for i in Text.split():\n",
    "    w.append(snow.stem(i))\n",
    "    \n",
    "Text3 = ' '.join(w)\n",
    "Text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75c3ad0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>slowmov aimless movi distress drift young man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "      <td>sure lost flat charact audienc near half walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>attempt artless black white clever camera angl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "      <td>littl music anyth speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "      <td>1</td>\n",
       "      <td>best scene movi gerard tri find song keep run ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  a very, very, very slow-moving, aimless movie ...          0   \n",
       "1  not sure who was more lost - the flat characte...          0   \n",
       "2  attempting artiness with black & white and cle...          0   \n",
       "3       very little music or anything to speak of.            0   \n",
       "4  the best scene in the movie was when gerardo i...          1   \n",
       "\n",
       "                                               clean  \n",
       "0      slowmov aimless movi distress drift young man  \n",
       "1      sure lost flat charact audienc near half walk  \n",
       "2  attempt artless black white clever camera angl...  \n",
       "3                            littl music anyth speak  \n",
       "4  best scene movi gerard tri find song keep run ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['clean'] = imdb['clean'].apply(lambda x: ' '.join([snow.stem(word) for word in x.split()]))\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bf6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "054700a4",
   "metadata": {},
   "source": [
    "### Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d21f2163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Because of problem with her eyesight, rey the African penguin had issue with swimming. That’s unusual for a penguin, and presented a big challenge for our aviculture team to help Rey overcome her hesitancy. Slowly and steadily, we trained her to be comfortable feeding in the water like the rest of the penguin colony. The aviculturists also trained Rey to accept daily eye drop from them a part of her special health care. Rey already had good relationship with some staff, and wa comfortable with them handling her. Senior Aviculturist Kim Fukuda say the team built on those bond to get Rey used to receiving the eye drops. \"She know the routine,\" Kim says. \"I usually give her the eye drop in one area of the exhibit after all the penguin get their vitamins. When that happens, she run over there and wait for me.\" Rosa, our oldest sea otter, ha very limited eyesight, among other health issues. The sea otter team had already trained Rosa so they could examine her eyes, and built on that trust to include administering the eye drop she needs.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "words= []\n",
    "for word in Text.split():\n",
    "    words.append(lemma.lemmatize(word))\n",
    "    \n",
    "Text_1 = ' '.join(words)\n",
    "Text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705a876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2a2f08f",
   "metadata": {},
   "source": [
    "### Parts of speech POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60a24778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin' \n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96c887c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mr.', 'NNP'),\n",
       " ('Trump', 'NNP'),\n",
       " ('became', 'VBD'),\n",
       " ('president', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('winning', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('political', 'JJ'),\n",
       " ('election', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Though', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('lost', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('support', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('some', 'DT'),\n",
       " ('republican', 'JJ'),\n",
       " ('friends', 'NNS'),\n",
       " (',', ','),\n",
       " ('Trump', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('friends', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('President', 'NNP'),\n",
       " ('Putin', 'NNP')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_pos_tagged = nltk.pos_tag(word_tokenize(sentence))\n",
    "nltk_pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53b6ceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>became</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>president</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>winning</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>political</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>election</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Though</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lost</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>support</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>some</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>republican</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>friends</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Trump</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>friends</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>President</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Putin</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word POS tag\n",
       "0          Mr.     NNP\n",
       "1        Trump     NNP\n",
       "2       became     VBD\n",
       "3    president      NN\n",
       "4        after      IN\n",
       "5      winning     VBG\n",
       "6          the      DT\n",
       "7    political      JJ\n",
       "8     election      NN\n",
       "9            .       .\n",
       "10      Though      IN\n",
       "11          he     PRP\n",
       "12        lost     VBD\n",
       "13         the      DT\n",
       "14     support      NN\n",
       "15          of      IN\n",
       "16        some      DT\n",
       "17  republican      JJ\n",
       "18     friends     NNS\n",
       "19           ,       ,\n",
       "20       Trump     NNP\n",
       "21          is     VBZ\n",
       "22     friends     NNS\n",
       "23        with      IN\n",
       "24   President     NNP\n",
       "25       Putin     NNP"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame(nltk_pos_tagged,\n",
    "                     columns=['Word','POS tag'])\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ac58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c6d27f",
   "metadata": {},
   "source": [
    "### Named Entity Recognition NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86e0d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.0-cp38-cp38-win_amd64.whl (12.1 MB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.2.1.tar.gz (173 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-win_amd64.whl (113 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-win_amd64.whl (452 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Building wheels for collected packages: langcodes\n",
      "  Building wheel for langcodes (setup.py): started\n",
      "  Building wheel for langcodes (setup.py): finished with status 'done'\n",
      "  Created wheel for langcodes: filename=langcodes-3.2.1-py3-none-any.whl size=169379 sha256=e4b4e646b7e457aeff6f4df7b3f84496b87c80461c88db6eda324a6d135caf81\n",
      "  Stored in directory: c:\\users\\tsharmili\\appdata\\local\\pip\\cache\\wheels\\aa\\5b\\45\\2e2ebdd7b888ba82dac447122f8ea0f4a2404910516d486679\n",
      "Successfully built langcodes\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 langcodes-3.2.1 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.0 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "265459d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\tsharmili\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-09 11:43:45.269572: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-11-09 11:43:45.269859: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0d17859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67e24265",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text_nlp = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76f75bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mr.', ''),\n",
       " ('Trump', 'PERSON'),\n",
       " ('became', ''),\n",
       " ('president', ''),\n",
       " ('after', ''),\n",
       " ('winning', ''),\n",
       " ('the', ''),\n",
       " ('political', ''),\n",
       " ('election', ''),\n",
       " ('.', ''),\n",
       " ('Though', ''),\n",
       " ('he', ''),\n",
       " ('lost', ''),\n",
       " ('the', ''),\n",
       " ('support', ''),\n",
       " ('of', ''),\n",
       " ('some', ''),\n",
       " ('republican', 'NORP'),\n",
       " ('friends', ''),\n",
       " (',', ''),\n",
       " ('Trump', 'ORG'),\n",
       " ('is', ''),\n",
       " ('friends', ''),\n",
       " ('with', ''),\n",
       " ('President', ''),\n",
       " ('Putin', 'PERSON')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ (word.text,word.ent_type_) for word in text_nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c14a681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Mr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " became president after winning the political election. Though he lost the support of some \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    republican\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " friends, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is friends with President \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Putin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(text_nlp , style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1da009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bce4392",
   "metadata": {},
   "source": [
    "### textual info ----> numerical info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer and TFIDF Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b9b989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = 'This is a very good and plain paper. This is really \\                    good and interesting'\n",
    "\n",
    "doc2= 'This paper is very interesting,  awesome'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7214e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79ee0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    tokens= word_tokenize(sent.lower())\n",
    "    stop_updated = stopwords.words('english')+list(punctuation)\n",
    "    final_word = [term for term in tokens if term not in stop_updated and len(term)>2]\n",
    "    res= ' '.join(final_word)\n",
    "    return res\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0860c914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good plain paper really good interesting'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1 = clean_text(doc1)\n",
    "doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cab92d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper interesting awesome'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_2 = clean_text(doc2)\n",
    "doc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d67b31c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good plain paper really good interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper interesting awesome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Text\n",
       "0  good plain paper really good interesting\n",
       "1                 paper interesting awesome"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pd.DataFrame([doc_1,doc_2],columns=['Text'])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b0eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc0be0c7",
   "metadata": {},
   "source": [
    "### Count Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9080b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22194d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "485e28ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= count_vect.fit_transform(doc['Text'])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225cfc61",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b60858a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome', 'good', 'interesting', 'paper', 'plain', 'really']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "185e3722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 1,\n",
       " 'plain': 4,\n",
       " 'paper': 3,\n",
       " 'really': 5,\n",
       " 'interesting': 2,\n",
       " 'awesome': 0}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcf254",
   "metadata": {},
   "source": [
    "#### Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "55f664c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>good</th>\n",
       "      <th>interesting</th>\n",
       "      <th>paper</th>\n",
       "      <th>plain</th>\n",
       "      <th>really</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  good  interesting  paper  plain  really\n",
       "0        0     2            1      1      1       1\n",
       "1        1     0            1      1      0       0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM= pd.DataFrame(X.toarray(), columns=count_vect.get_feature_names())\n",
    "DTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b549d7",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#units within a sentence \n",
    "# bigrams\n",
    "# trigrams\n",
    "# how many words taken each time and the frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b0f12",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b562989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_bg = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbd17da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bg = count_vect_bg.fit_transform(doc['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7c15ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good interesting',\n",
       " 'good plain',\n",
       " 'interesting awesome',\n",
       " 'paper interesting',\n",
       " 'paper really',\n",
       " 'plain paper',\n",
       " 'really good']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_bg.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "215d8bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good interesting</th>\n",
       "      <th>good plain</th>\n",
       "      <th>interesting awesome</th>\n",
       "      <th>paper interesting</th>\n",
       "      <th>paper really</th>\n",
       "      <th>plain paper</th>\n",
       "      <th>really good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good interesting  good plain  interesting awesome  paper interesting  \\\n",
       "0                 1           1                    0                  0   \n",
       "1                 0           0                    1                  1   \n",
       "\n",
       "   paper really  plain paper  really good  \n",
       "0             1            1            1  \n",
       "1             0            0            0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM_bg= pd.DataFrame(X_bg.toarray(), columns=count_vect_bg.get_feature_names())\n",
    "DTM_bg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e204299",
   "metadata": {},
   "source": [
    "Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4c4b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_ubg = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56d85add",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ubg = count_vect_ubg.fit_transform(doc['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23e5856c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome',\n",
       " 'good',\n",
       " 'good interesting',\n",
       " 'good plain',\n",
       " 'interesting',\n",
       " 'interesting awesome',\n",
       " 'paper',\n",
       " 'paper interesting',\n",
       " 'paper really',\n",
       " 'plain',\n",
       " 'plain paper',\n",
       " 'really',\n",
       " 'really good']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_ubg.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f05e6aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>good</th>\n",
       "      <th>good interesting</th>\n",
       "      <th>good plain</th>\n",
       "      <th>interesting</th>\n",
       "      <th>interesting awesome</th>\n",
       "      <th>paper</th>\n",
       "      <th>paper interesting</th>\n",
       "      <th>paper really</th>\n",
       "      <th>plain</th>\n",
       "      <th>plain paper</th>\n",
       "      <th>really</th>\n",
       "      <th>really good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  good  good interesting  good plain  interesting  \\\n",
       "0        0     2                 1           1            1   \n",
       "1        1     0                 0           0            1   \n",
       "\n",
       "   interesting awesome  paper  paper interesting  paper really  plain  \\\n",
       "0                    0      1                  0             1      1   \n",
       "1                    1      1                  1             0      0   \n",
       "\n",
       "   plain paper  really  really good  \n",
       "0            1       1            1  \n",
       "1            0       0            0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM_ubg= pd.DataFrame(X_ubg.toarray(), columns=count_vect_ubg.get_feature_names())\n",
    "DTM_ubg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bcb9f7",
   "metadata": {},
   "source": [
    "Trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bdc00c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good plain paper</th>\n",
       "      <th>paper interesting awesome</th>\n",
       "      <th>paper really good</th>\n",
       "      <th>plain paper really</th>\n",
       "      <th>really good interesting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good plain paper  paper interesting awesome  paper really good  \\\n",
       "0                 1                          0                  1   \n",
       "1                 0                          1                  0   \n",
       "\n",
       "   plain paper really  really good interesting  \n",
       "0                   1                        1  \n",
       "1                   0                        0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_tg = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "X_tg = count_vect_tg.fit_transform(doc['Text'])\n",
    "\n",
    "DTM_tg= pd.DataFrame(X_tg.toarray(), columns=count_vect_tg.get_feature_names())\n",
    "DTM_tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e490fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "715cd01f",
   "metadata": {},
   "source": [
    "Specify max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "428f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nt = CountVectorizer(max_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a407251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'interesting', 'paper']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nt = count_nt.fit_transform(doc['Text'])\n",
    "\n",
    "count_nt.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "92b452ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>interesting</th>\n",
       "      <th>paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good  interesting  paper\n",
       "0     2            1      1\n",
       "1     0            1      1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM_nt= pd.DataFrame(X_nt.toarray(), columns=count_nt.get_feature_names())\n",
    "DTM_nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60412dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('imdb_sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dce9c89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>slow-moving aimless movie distressed drifting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "      <td>sure lost flat characters audience nearly half...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "      <td>attempting artiness black white clever camera ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "      <td>little music anything speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "      <td>best scene movie gerardo trying find song keep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  A very, very, very slow-moving, aimless movie ...          0   \n",
       "1  Not sure who was more lost - the flat characte...          0   \n",
       "2  Attempting artiness with black & white and cle...          0   \n",
       "3       Very little music or anything to speak of.            0   \n",
       "4  The best scene in the movie was when Gerardo i...          1   \n",
       "\n",
       "                                               clean  \n",
       "0  slow-moving aimless movie distressed drifting ...  \n",
       "1  sure lost flat characters audience nearly half...  \n",
       "2  attempting artiness black white clever camera ...  \n",
       "3                        little music anything speak  \n",
       "4  best scene movie gerardo trying find song keep...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['review'].apply(lambda x: clean_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "613390aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "X_df = count_vect.fit_transform(df['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56a805f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>10 10</th>\n",
       "      <th>10 grade</th>\n",
       "      <th>10 scale</th>\n",
       "      <th>10 score</th>\n",
       "      <th>10 setting</th>\n",
       "      <th>13</th>\n",
       "      <th>13 film</th>\n",
       "      <th>13 rating</th>\n",
       "      <th>15pm</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yun</th>\n",
       "      <th>yun fat</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zillion times</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie movies</th>\n",
       "      <th>zombie students</th>\n",
       "      <th>zombiez</th>\n",
       "      <th>zombiez part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  10 10  10 grade  10 scale  10 score  10 setting  13  13 film  \\\n",
       "0   0      0         0         0         0           0   0        0   \n",
       "1   0      0         0         0         0           0   0        0   \n",
       "2   0      0         0         0         0           0   0        0   \n",
       "3   0      0         0         0         0           0   0        0   \n",
       "4   0      0         0         0         0           0   0        0   \n",
       "\n",
       "   13 rating  15pm  ...  youtube  yun  yun fat  zillion  zillion times  \\\n",
       "0          0     0  ...        0    0        0        0              0   \n",
       "1          0     0  ...        0    0        0        0              0   \n",
       "2          0     0  ...        0    0        0        0              0   \n",
       "3          0     0  ...        0    0        0        0              0   \n",
       "4          0     0  ...        0    0        0        0              0   \n",
       "\n",
       "   zombie  zombie movies  zombie students  zombiez  zombiez part  \n",
       "0       0              0                0        0             0  \n",
       "1       0              0                0        0             0  \n",
       "2       0              0                0        0             0  \n",
       "3       0              0                0        0             0  \n",
       "4       0              0                0        0             0  \n",
       "\n",
       "[5 rows x 9226 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM_df = pd.DataFrame(X_df.toarray(),columns=count_vect.get_feature_names())\n",
    "DTM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c701198c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 9226)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa85880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25cf8a0b",
   "metadata": {},
   "source": [
    "### Term Frequency -Inverse Document Frequency TF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3be90e8a",
   "metadata": {},
   "source": [
    "t- term(word)\n",
    "d- document(set of word)\n",
    "N- count of corpus\n",
    "corpus- total document set\n",
    "\n",
    "TF - measures the frequency of a wors in a doc\n",
    "\n",
    "Document Frequency: It measures the importance of the documents\n",
    "\n",
    "IDf stpowords get low importance and relative weightage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9fcab4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d6973db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a5817c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tfidf = tfidf.fit_transform(doc['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "152148f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>good</th>\n",
       "      <th>interesting</th>\n",
       "      <th>paper</th>\n",
       "      <th>plain</th>\n",
       "      <th>really</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755256</td>\n",
       "      <td>0.268685</td>\n",
       "      <td>0.268685</td>\n",
       "      <td>0.377628</td>\n",
       "      <td>0.377628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome      good  interesting     paper     plain    really\n",
       "0  0.000000  0.755256     0.268685  0.268685  0.377628  0.377628\n",
       "1  0.704909  0.000000     0.501549  0.501549  0.000000  0.000000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ = pd.DataFrame(X_tfidf.toarray(),columns=tfidf.get_feature_names())\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ef3a3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_nt = TfidfVectorizer(max_features=3)\n",
    "X_tnt = tfidf_nt.fit_transform(doc['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2fe4cfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'interesting', 'paper']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_nt.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "589bacea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acting</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>one</th>\n",
       "      <th>really</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.825602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>748 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       acting  bad  film  good  great  like     movie  one  really  time\n",
       "0    0.000000  0.0   0.0   0.0    0.0   0.0  1.000000  0.0     0.0   0.0\n",
       "1    0.000000  0.0   0.0   0.0    0.0   0.0  0.000000  0.0     0.0   0.0\n",
       "2    0.825602  0.0   0.0   0.0    0.0   0.0  0.564253  0.0     0.0   0.0\n",
       "3    0.000000  0.0   0.0   0.0    0.0   0.0  0.000000  0.0     0.0   0.0\n",
       "4    0.000000  0.0   0.0   0.0    0.0   0.0  1.000000  0.0     0.0   0.0\n",
       "..        ...  ...   ...   ...    ...   ...       ...  ...     ...   ...\n",
       "743  0.000000  0.0   0.0   0.0    0.0   0.0  0.000000  0.0     0.0   0.0\n",
       "744  0.000000  0.0   1.0   0.0    0.0   0.0  0.000000  0.0     0.0   0.0\n",
       "745  0.000000  0.0   0.0   0.0    0.0   0.0  0.000000  0.0     0.0   0.0\n",
       "746  0.000000  1.0   0.0   0.0    0.0   0.0  0.000000  0.0     0.0   0.0\n",
       "747  0.000000  0.0   0.0   0.0    0.0   0.0  0.000000  1.0     0.0   0.0\n",
       "\n",
       "[748 rows x 10 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_nt_imdb = TfidfVectorizer(ngram_range=(1,2), max_features=10)\n",
    "\n",
    "X_imdb = tfidf_nt_imdb.fit_transform(df['clean'])\n",
    "\n",
    "dtm_imdb = pd.DataFrame(X_imdb.toarray(),columns=tfidf_nt_imdb.get_feature_names())\n",
    "dtm_imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034b3c4",
   "metadata": {},
   "source": [
    "### Text preprocessing- numerical format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a06e4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df['clean']\n",
    "y=df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4205e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "250c3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11670b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_bow = tfidf.fit_transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7231c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_bow = tfidf.transform(xtest)# we take the already fit model and perform the operation on testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b053ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7886ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(xtrain_bow,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7fd29129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(xtest_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6570293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6ed21c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7733333333333333"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytest , pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "52380e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43, 21],\n",
       "       [13, 73]], dtype=int64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aafde14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72        64\n",
      "           1       0.78      0.85      0.81        86\n",
      "\n",
      "    accuracy                           0.77       150\n",
      "   macro avg       0.77      0.76      0.76       150\n",
      "weighted avg       0.77      0.77      0.77       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71b46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c0846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ce227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252b08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
